README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. 

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	Zip up the files cool.flex, test.cl, README, and test.output.
	Don't forget to edit the README file to include your write-up, 
	and to write your own test cases in test.cl. Submit your zip
	file through Canvas.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------
Design decisions:
I did not feel the need to define any new constants to make the lexer work. I decided that curr_lineno, string_buf, and string_buf_ptr were all I was going to need.
I decided to list all of the specific (and short) regular expressions first in the regex section. This was to ensure that a keyword or special character would match as its specific instance and the correct token would be passed on instead of a more generic regex (and thus, generic token.)

Most of my regular expressions are straightforward, as they simply do what the COOL documentation says (ex: an object ID is a lowercase letter followed by any amount of letters, numbers, or underscores.) There were three that were not simply translated from the COOL documentation, so I will talk about those in more detail.

The first to occur in my file is LEFTOVERS. This token is my catch-all token for any character that is missed. The project specification noted that all characters must be accounted for, and any invalid character must be captured and pass on a character message. I use the LEFTOVERS regular expression and consequent return code to do this.
The other two are block comments and strings. I implemented both of these using flex start conditions, as I felt that that gave me more flexibility in dealing with special cases inside of a comment or string.
The comment start condition code is fairly simple: it is entered when a "(*" is detected, and exited when a "*)" is found. While inside a comment, there are three things that can happen: regular comment text is found, a newline occurs, and and EOF is found. I have two cases for the regular text (to deal with *'s and other text), one for a newline (to increment the line number) and one for EOF (to return the error).
The string start condition code is slightly more complicated than the comment code because there are more special cases within a string.
The start condition is when a " is found, and it exits when an error occurs or the following " is found. While in the code for strings, I check for the various string errors (unterminated string constant, eof) and handle escaped characters. All valid characters are added to the string buffer, and when the string is closed, the buffer will be the string that needs to be added to the string table. 



Why my code is correct:
I believe that my code is correct because of my process in creating it and my testing after the fact. I worked through the COOL documentation and project description to ensure that I accounted for all of the different types of return tokens, and then tested for each token type and each error that I found, and compared that output with the correct lexer.



Why the test cases are adequate:
The test.cl file that we were given contains one of each of the token types that could be generated on its own, so my test cases at the bottom of test.cl focus on testing that errors or abnormal input are handled correctly.
I was only allowed to have one EOF within a comment or string, so I tested EOF in a comment and included my test.cl that has an end of file in the middle of a string.
My code at the bottom of test.cl tests an unterminated string, a string with a properly escaped newline, a long string, EOF in a string constant, and a "*)" outside of a comment. It also tests that \0 is valid. I was not able to figure out how to test the null character, but other than that, I have tested all of the other boundary conditions, and my lexer produces the same output as the correct lexer binary does.
